//
// Copyright (c) 2020 NVI, Inc.
//
// This file is part of the FSL10 Linux distribution.
// (see http://github.com/nvi-inc/fsl10).
//
// This program is free software: you can redistribute it and/or modify
// it under the terms of the GNU General Public License as published by
// the Free Software Foundation, either version 3 of the License, or
// (at your option) any later version.
//
// This program is distributed in the hope that it will be useful,
// but WITHOUT ANY WARRANTY; without even the implied warranty of
// MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
// GNU General Public License for more details.
//
// You should have received a copy of the GNU General Public License
// along with this program. If not, see <http://www.gnu.org/licenses/>.
//

= RAID Notes for FSL 10
J.F.H. Quick, W.E. Himwich, and D.E. Horsley
Version 1.3.2 - November 2020

:sectnums:
:experimental:
:downarrow: &darr;

:toc:
<<<
== Introduction

These notes are intended to cover, albeit tersely, the major issues
for RAID operations with FSL10 (see the <<installation.adoc#,FSL10
      Installation>> document). The disk lay-out has changed
significantly since FSL9, which required updating the scripts that are
used. In addition, the scripts have been extensively revised to
provide more protection from possible errors in how they are used.

All operations and scripts in this document require *root* privileges
unless otherwise indicated.

== Guidelines for RAID operations

The FSL10 RAID configuration normally uses two disks configured
according to the FSL10 installation instructions (see the <<installation.adoc#,FSL10
      Installation>> document). Below are mandatory
and recommended guidelines.

=== Mandatory practices

These practices are fundamental to the operation of the RAID.

. Never mix disks from different computers in one computer.
. Never split up a RAID pair unless already synced, check with *mdstat*
 
A RAID pair (kept together and in order) can be removed or moved
between computers if need be. A disk rotation (or initializing a new
    disk)  is probably the only reason to split a pair.

NOTE: When booting a disk from a RAID by itself, you may see
~20 of the *volume group
not found* error messages, then the machine will boot. These error
messages  only appear like this the first time a disk
from the RAID is booted without its partner.

=== Recommended practices

These recommendations are intended to provide consistent procedures and make it easier to understand any issues, if they occur.

. Always use the lower number controller (slot) as the *primary*, label it as such, and the other slot as *secondary*
. Make the upper (or left) slot the *primary*, the lower (or right) slot the *secondary*. If necessary, change the internal cabling to make it so.
. Always boot for a refresh/blank with the *primary* slot turned on and the *secondary* slot turned off: so it is clear which is the active disk
. Label the disks (so visible when in use) with the system name and number them 1, 2, and 3, ...
. Label the disks (so visible when in use) with their serial numbers, either from *mdstat* when only one disk inserted or by examining the disk
. For reference, place the disk serial numbers in a file with their corresponding numbers, e.g.:

+
./root/DISKS.txt
[source]
```
1=ZC1B1YCC
2=ZC1A6WZ1
3=ZC1AHENM
```

. When rotating disks, keep the disks in cyclical order (*primary*, *secondary*, *shelf*): 1, 2, 3; then 2, 3, 1;, then 3, 1, 2, then 1, 2, 3; and so on
. Rotate disks for a given computer at least once a month, and before any updates
. If you have a spare computer (and/or additional systems), keep the disks in the same sequence on all the computers
. Do not turn a disk off while the system is running. The only time a key switch state should be changed while the system is running is to add a disk for a blank or refresh operation.

== Disk Rotation

This section describes the disk rotation procedure. It is used to make
periodic updates of the shelf disk.

NOTE: Your BIOS must be set to allow hot swapping of disks, particularly for the *secondary* controller,

TIP: If you do not have access to the *root* account, you may
have *sudo* access to the privileged commands. If so, you will need
to run the *shutdown -h now* and *refresh_secondary* commands with
*sudo*.  This is true for other privileged commands used elsewhere in
this document.

. Wait until the RAID is not recovering, check with *mdstat*
. Shutdown the system, e.g., *shutdown -h now*
. Take the disk from *primary* slot, put it on the *shelf*
+

We recommend that you label the disk immediately, including the date
(and possibly the time). In addition to getting the disk labeled
before it is put away, it will reduce the chances that it is confused
with the _old_ *shelf* disk.

. Move the disk from the *secondary* slot to the *primary* slot, keyed-on
. Move the _old_ *shelf* disk to the *secondary* slot, keyed-off
. Boot (*primary* keyed-on, *secondary* keyed-off)
. Login in as root
. Run *refresh_secondary*
. Key-on the *secondary* slot when prompted
. If the script rejects the disk (and stops with an error), seek expert advice. Be sure to note any messages so they can be reported.
. If the disk is accepted, let the refresh run to completion (the progress report can be aborted as indicated). The system can be used in the meantime, but may be a little slow.

== Recoverable testing

This section describes a method for testing updates in a way that provides a
relatively easy recovery option if a problem occurs. Should that recovery fail
for some reason, it is still possible to recover with the shelf disk as
described in the <<Recover from a shelf disk>> section below.

Seek expert advice for this, but the basic plan is given below:

TIP: If you are testing a
<<installation.adoc#_kernel_updates,kernel update>>, please be sure to
follow the steps in the three additional **TIP**s included below.

. If a rotation hasn't just been completed, perform one (as an extra backup)
. Before proceeding verify that there is no recovery in progress, check with *mdstat*
. Drop the *primary* disk out of the RAID (as a backup) by running *drop_primary*
. Install and test the update
+

[TIP]
====

If you are testing a kernel update, you must to take extra steps
between installing and testing the update. After installing it:

.. Shutdown the system
.. Turn off the disk in the primary slot
.. Reboot

Then proceed to testing.

====


If the update is deemed *successful*:

[start=5]
. Run *recover_raid* to add the *primary* disk back into the RAID.
+

[TIP]
====

If you are testing a kernel update, you must to turn on the disk
in the primary slot again before *recover_raid* can succeed.

====

. Once the recovery completes (this may only take a few minutes), the system has been successfully updated.
+

[TIP]
====

If you are testing a kernel update, you must to reboot after the
recovery completes to return the disk in the primary slot to being
*sda*.

====

Alternatively, if the update is deemed to have *failed*, the system can be
 recovered as follows:

[start=5]
. Shutdown the system, e.g., *shutdown -h now*
. Key-off the *secondary* slot
. Reboot (*primary* keyed-on, *secondary* keyed-off)
. Run *blank_secondary*
. Key-on the *secondary* slot when prompted
. Answer *y* to blank
. Run *refresh_secondary*
. Once the refresh is complete (this may take several hours), you have recovered to the original state.

== Recover from a shelf disk

The section describes how to recover from a _good_ shelf disk. This
might be needed, e.g., if it is discovered that a problem has
developed on the RAID pair since the last disk rotation, possibly due
to a bad update of some type or some other error.

TIP: Before using this procedure, it should be considered whether the
damage is extensive enough to require starting over from the shelf
disk or whether it can be reasonably repaired in place.

IMPORTANT: This will only produce good result if the shelf disk is in
_good_ copy.

WARNING: Do *not* use this procedure if a problem with computer caused
the damage to the RAID.

NOTE: Your BIOS must be set to allow hot swapping of disks,
    particularly for the *secondary* controller,

. Shutdown the system, e.g., *shutdown -h now*
. Take the disks from both the *primary* and *secondary* slots, set them aside.
. Insert the _good_ shelf disk in the *primary* slot, keyed-on.
. Insert the disk that is next in cyclic order (from the ones set aside)  in the *secondary* slot, keyed-off.
. Reboot (*primary* keyed-on, *secondary* keyed-off)
. Login in as root
. Run *blank_secondary*
. Key-on the *secondary* slot when prompted
. Answer *y* to blank
. Run *refresh_secondary*
+
Once the refresh has entered the recovery phase (the progress display is being shown onscreen), the system can
be used for operations, if need be. In that case, the rest of this procedure can be completed when time allows.
. Wait until the RAID is not recovering, check with *mdstat*
. Shutdown the system, e.g., *shutdown -h now*
. Take the disk from *primary* slot, put it on the *shelf*
. Move the disk from the *secondary* slot to the *primary* slot, keyed-on
. Insert the remaining disk, that was set aside, in the *secondary* slot, keyed-off.
. Reboot (*primary* keyed-on, *secondary* keyed-off)
. Login in as root
. Run *blank_secondary*
. Key-on the *secondary* slot when prompted
. Answer *y* to blank
. Once the refresh is complete, you have recovered to the state of the previous _good_ shelf disk.

== Initialize a new disk

If one or more of the disks in the set for the RAID fails, you can
initialize new ones to replace them.

IMPORTANT: The new disks should be at least
as large as the smallest of the remaining disks.

The sub-sections below cover various scenarios for intializing one new
disk to complete a set of three, i.e., one of three disks in a set has
failed.  It is assumed that you want to maintain the cyclic numbering
of the disks for rotations (but that is not required).  It should be
straightforward to adapt them to other cases.

If you need to initilaize more than one disk, please follow the
instructions in the <<installation.adoc#_setup_additional_disk,Setup
additional disk>> section of the FSL10 Installation document.

=== Currently two disks are running in the RAID

This case corresponds to not having a good shelf disk.

. Wait until the RAID is not recovering, check with *mdstat*
. Shutdown the system, e.g., *shutdown -h now*

If the disks are in cyclical order (i.e, primary, secondary are numbered
    in order: 1, 2, or 2, 3, or 3, 1), you should:

. Take the disk from *primary* slot, put it on the *shelf*
. Move the disk from the *secondary* slot to the *primary* slot, keyed-on

If the disks are
not in cyclical order (i.e, primary,
      secondary are numbered in order: 1, 3, or 2, 1, or 3, 2), you
    should:
    
. Take the disk from *secondary* slot, put it on the *shelf*
    
In either case, finish with:

. Put the new disk in the *secondary* slot, key-off.
. Boot (*primary* keyed-on, *secondary* keyed-off)
. Login in as *root*
. Run *blank_secondary*
. Key-on the *secondary* slot when prompted
. Answer *y* to blank
. Run *refresh_secondary*
. Once the refresh is complete, the disk can be used normally
. Label the new disk with its system name, number, and serial number.

=== Currently one disk is running in the RAID, but two are installed

In this case, there is a good shelf disk. The strategy used avoids overwriting it until there are three functional disks again.

. Use *mdstat* to determine which disk is running, compare the serial number to those shown on the labels or inspect the disks to determine their serial numbers.
. Shutdown the system, e.g., *shutdown -h now*
. Remove the non-working disk.
. Move the working disk to the *primary* slot, if it isn't already there, keyed-on.
. Put the new disk in the *secondary* slot, keyed-off.
. Boot (*primary* keyed-on, *secondary* keyed-off)
. Login in as *root*
. Run *blank_secondary*
. Key-on the *secondary* slot when prompted
. Answer *y* to blank
. Run *refresh_secondary*
. Once the refresh is complete, the disk can be used normally
. Label the new disk with its system name, number, and serial number.

If the disks are not in cyclical order (i.e., primary, secondary are
numbered in order: 1, 3, or 2, 1, or 3, 2), on the next disk rotation
you should move the *secondary* disk to the shelf instead of moving
the *primary*.

=== Currently one disk is installed and running

In this case, the shelf disk is assumed to be healthy, but older.
 Again, the strategy is to avoid overwriting it until there is a full
 complement of disks available.

If the working disk is not in the *primary* slot:

. Shutdown the system, e.g., *shutdown -h now*
. Move the working disk to the *primary* slot, keyed-on.
. Boot (*primary* keyed-on, *secondary* empty)

Then in any event:

. Put the new disk in the *secondary* slot, keyed-off.
. Login in as *root*
. Run *blank_secondary*
. Key-on the *secondary* slot when prompted
. Answer *y* to blank
. Run *refresh_secondary*
. Once the refresh is complete, the disk can be used normally
. Label the new disk with its system name, number, and serial number.

If the disks are not in not in cyclical order (i.e, primary, secondary
are numbered in order, 1, 3, or 2, 1, or 3, 2), on the next disk
rotation you should move the *secondary* the shelf in stead of the
*primary*.

== Script descriptions

This section describes the various scripts that are used for RAID maintenance.

=== mdstat

This script can be used by any user (not just *root*) to check the
status of the RAID. It is most useful for checking whether a recovery
is in process or has ended, but is also useful for showing the current
state of the RAID, including any anomalies.

The script also lists various useful details for all block devices (such
as disks) that are currently connected, including their model and serial
numbers where applicable.

=== refresh_secondary

This can be used to refresh a *shelf* disk for the RAID as a new
*secondary* disk (*sdb*) as part of a standard three (or more) disk
rotation.

Initially, the script performs some sanity checks to confirm that the
RAID */dev/md0*:

. Exists.
. Is not a clean state, i.e., it needs recovery.
. Is not already recovering, i.e., is in a recoverable state.

Additional checks are performed to confirm that the content the script
intends to copy is where it expects it to be and has the right form.
Any *primary* disk (*sda*) will be rejected that:

. Is not part of the RAID (*md0*)
. Has a boot scheme other than the BIOS or UEFI set up as described in the FSL10 Installation Document.

If the *primary* disk is removable, the user will be provided with some
information about the disk and given an opportunity to continue with
kbd:[Enter] or abort with kbd:[Ctrl+C].  Typically, if a USB disk is
identified as the *primary*, one would not want to continue. However
for some machines the SATA disk that is the *primary* may be marked
removable if it is hot swappable, but would still be appropriate to
use. 

For safety reasons, to ensure that only an old *shelf* disk is overwritten,
any *secondary* disk (*sdb*) will be rejected that:

. Was loaded (slot keyed-on) before starting the script
+
Unless overridden by *-A* or previously loaded by this or the *blank_secondary* script.

. Is already part of RAID *md0*

+
Which should only happen if run incorrectly with *-A* (or other
interfering commands have been executed) or the disk has
fallen out of the RAID due to failure.

. Has a RAID from a different computer, i.e., foreign
+
Technically this could also be another RAID from the same computer, but not of a
properly set up FSL10 computer, which should have only the one RAID

. Has any part already mounted
+
Again catching misuse of the *-A* option.

. Has a different boot scheme than the *primary*
+
And hence is probably from a different computer.

. Has a different RAID UUID
+
This would be a disk from a different computer. Though whether this
check can actually trigger after the test for a foreign RAID above
remains to be seen.

. Was last booted at a future *TIME* (possibly due to a mis-set clock or clocks)
. Has a higher *EVENT* count, i.e., is newer (but see the *WARNING* item below)
. Has been used (booted) separately by itself
. Has a different partition layout from the *primary*
. Is smaller than the size of the RAID on the primary disk.

If any of the checks reject the disk, we recommend you seek expert
advice; please record the error so it can be reported.

The checks are included to make the refresh process as safe as
possible, particular at a station with more than one FSL__x__ computer.
We believe all the most common errors are trapped, but the script
should still be used with care.

WARNING: The check on the *EVENT* counter is intended to prevent accidentally using
the *shelf* disk to overwrite a newer disk from the RAID.  This check can be
over-run if the *primary* has run for a considerable period of time
before the refresh is attempted.  This should not be an issue if the
refresh is attempted promptly after the *shelf* disk is booted for the
first time by itself and the RAID was run on the other disks for more than a trivial
amount of time beforehand.

If the disk being refreshed is from the same computer and has just
been on the *shelf* unused since it was last rotated, it is safe to
refresh and should be accepted by all the checks. In other words,
        normal disk rotation should work with no problems.

If the *secondary* disk is removable, the user will be provided with some
information about the disk and given an opportunity to continue with
kbd:[Enter] or abort with kbd:[Ctrl+C].  Typically, if a USB disk is
identified as the *secondary*, one would not want to continue. However
for some machines the SATA disk that is the *secondary* may be marked
removable if it is hot swappable, but would still be appropriate to
use. 

This script requires the *secondary* disk (*sdb*) to not be loaded, i.e.,
the slot turned off, when the script is started. However, it has an
option, *-A* (use only with expert advice), to "Allow" an already
loaded disk to be used. It is intended to make remote operation
possible and must be used with extra care.

If the disk is turned on (when prompted) during the script, it
will automatically be "Allowed" by both this script and
*blank_secondary*, which also supports this feature.  This allows
(expert use only), after a failed *refresh_secondary*, running
*blank_secondary* then rerunning *refresh_secondary*, all without having to
*shutdown*, turn the disk off, reboot, start the script, and turn the disk on for each.

The refresh will take several hours. The script provides a progress
indicator that can safely be aborted (using kbd:[Ctrl+C] as described
    by the on-screen instructions) if that is preferred.  An active
screen saver may make it difficult to see the progress after awhile,
       but pressing kbd:[shift] or some other key should make it
       visible again.  If you abort the progress indicator, you can
       check the progress later with *mdstat*. The system can be used
       normally while it refreshing, but it may be a little slow.

Once the progress indicator is updating, it is safe to reboot the
computer if it is needed.

=== blank_secondary

This script should only be used with expert advice.

It can be used to make _any_ *secondary* disk (*sdb*) refreshable, if
it is big enough. It must be used with care and only on a *secondary*
disk that you know is safe to erase. Generally speaking you don't want
to use it with a disk from a different FSL__x__ computer, except for very
unusual circumstances, see <<Recovery scenarios>> section for some example
cases. It will ask you to confirm before blanking.

It will reject any *secondary* disk (*sdb*) that:

. Was loaded (slot keyed-on) before starting the script
+
Unless you have just loaded it through *refresh_secondary*'s auspices or used
the *-A* option to "Allow" it (see below).

. Is still part of the RAID *md0*
+
Which should only happen if run incorrectly with *-A* (or other
interfering commands have been executed).

. Has any partition already mounted
+
Again catching misuse of the *-A* option.

. Has a partition that is in RAID *md0*
+
This is essentially redundant with the item two above, but is included
out of an abundance of caution.

. Has a partition that is included in any RAID.

If the *primary* disk is removable, the user will be provided with some
information about the disk and given an opportunity to continue with
kbd:[Enter] or abort with kbd:[Ctrl+C].  Typically, if a USB disk is
identified as the *primary*, one would not want to continue. However
for some machines the SATA disk that is the *primary* may be marked
removable if it is hot swappable, but would still be appropriate to
use. 

This script requires the *secondary* disk (*sdb*) to not be loaded, i.e.,
the slot turned off, when the script is started. However, it has an
option, *-A* (use only with expert advice), to "Allow" an already
loaded disk to be used. It is intended to make remote operation
possible and must be used with extra care.

If the disk is turned on (when prompted) during the script, it will
automatically be "Allowed" by both this script and
*refresh_secondary*, which also supports this feature.  This allows
you to then run *refresh_secondary* immediately without having to 
*shutdown*, turn the disk off, reboot, start the script, and turn the disk on.

NOTE: On the 32-bit *i386* platform, due to a broken *vgremove* binary, this
script can give WARNINGs when erasing disks that were used for LVM.  These
warnings can safely be ignored - the disk will be successfully blanked (despite
*vgremove* having segmentation-faulted instead of performing the requisite
action thereby causing *pvremove* to complain about the VG still being active.)

=== drop_primary

This script is only for use with expert advice.

This script can be used to drop a *primary* disk (*sda*) out of a RAID pair
(by marking as failed) so that it can act as a safety backup during major
upgrades or other significant changes.

Initially, the script performs some sanity checks to confirm that the
RAID */dev/md0*:

. Exists.
. Is in a clean state, i.e., both disks are present and no recovery is
  currently in progress.
. Contains the *primary* disk (*sda*) as a member.

If the *primary* disk is removable, the user will be provided with some
information about the disk and given an opportunity to continue with
kbd:[Enter] or abort with kbd:[Ctrl+C].  Typically, if a USB disk is
identified as the *primary*, one would not want to continue. However
for some machines the SATA disk that is the *primary* may be marked
removable if it is hot swappable, but would still be appropriate to
use. 

NOTE: This script is non-destructive in nature and its effect can 
easily be reversed by running the *recover_raid* script mentioned
below.

=== recover_raid

This script is only for use with expert advice.

This script can be used to recover a disk (*sda* or *sdb*) that has
fallen out of the RAID array, becoming *inactive*.  A disk can _fall_ out of
the array for several possible reasons, including:

. A real disk fault of some sort, including one caused by turning it off
  whilst it is still in use.
. Use of the *mdadm* command with *-f* option to mark it as faulty.
. Turning it off whilst the system is shutdown and booting without it.

This script is designed to be used only with a
set of disks that were most recently used _together_ in an active
RAID.  It is recommended only to use this script if the key switches
for the disks have not been manipulated since the *inactive* disk fell
out of the RAID; in this case it should always be safe.

NOTE: The *inactive* disk is either *failed* or *missing*. It is
*failed* if it was either marked *failed* by hand or dropped out of the RAID due to disk errors.
It is *missing* if either the system was rebooted with the disk
*failed* or physically missing or it was manually marked _removed_.  You
can check which state an *inactive* disk is in  with
*mdadm{nbsp}--detail{nbsp}/dev/md0* -- which lists *failed* as
*faulty* but a missing disk will not appear at all.

NOTE: The *active* disk is the one the system is still running on.

TIP: It is okay to use this script even if the *inactive* disk fell out
the RAID a (long) long time ago (in a galaxy far, far away) and/or
there have been extensive changes to the *active* disk.
It is also okay to use if the system
was rebooted (even multiple times) or the *active* disk was used
(booted) separately by itself since the *inactive* disk fell out of the
RAID. 

WARNING: This script must *NOT* be used if the *inactive* disk has
been changed in any way e.g., by being used (booted) separately (which is
    caught by the script) or refreshed against some other disk, or if
the *active* disk has been used to refresh any other disk in the
interim.  In particular, the script must *NOT* be used to refresh a
*shelf* disk -- only use *refresh_secondary* for that purpose.

It normally works on *md0*, but a different *md* device can be specified as the first argument.

It will refuse to recover the RAID if the RAID:

. Does not need recovery
. Is not in a recoverable state, e.g., is already recovering

or if any *missing* disk:

[start=3]
. Has a later modification *TIME* than the *active* disk
. Has a higher *EVENT* count, i.e., is newer,  than the *active* disk
. Has been used (booted) separately (as mentioned above in the *WARNING* item)

or if no matching *missing* disk can be found.

The recovery may be fairly quick, as short as a few minutes, if the
*inactive* disk is relatively fresh.
There is an ongoing progress display that can be
terminated early with kbd:[Ctrl-C], without affecting the recovery.
If you abort the progress indicator, you can check the progress with *mdstat*. The
system can be used normally while it recovering, but it may be a
little slow.

=== refresh_spare_usr2

This script is not part of RAID operations per se, but is included in
this document for completeness. In a two computer configuration
(*operational* and *spare*), it is used to make a copy of the
*operational* computer's */usr2* partition on the *spare* computer.
Normally this partition holds all the operational FS programs and
data. The script can be found in */root/fsl10/RAID*.  Full
instructions for its installation are included in the script. The
script will give a warning about its use and prompt for permission to
proceed when it it is run.

WARNING: It should installed on the *spare* computer _only_.

WARNING: When this script is run, neither computer should have anyone logged in
with a home directory on */usr2* nor should there be any activity occurring that
will affect */usr2*.

NOTE: Despite what the script says, it is possible to run the script
by using *su* or *sudo* from a non-root account as long as there is no
activity involving */usr2* and the user's current directory is not on
*/usr2*. For the latter issue, *cd*-ing to */tmp* is a reasonable
choice. If after entering `y` to proceed, you are unceremoniously
logged out, it probably means you still had your current directory
somewhere on */usr2*. In this case, no harm was done. You can try
re-running the script, but this time please be sure to *cd* off
*/usr2* first.

IMPORTANT: For this script to work usefully, the *operational* and
*spare* computers should have the same set-up including particularly the
same user accounts with same UIDs and GIDs in parallel for all
accounts, particularly for those that have home directories on */usr2*,
  as well as other OS set-up information the FS may depends such as
  */etc/hosts* and */etc/ntp.conf*.

IMPORTANT: It is recommended that the script be used (including for initial testing)
  immediately after a disk rotation to provide the ample opportunities
  for recovery if there is a problem. In particular, for initial
  testing the procedure in the <<Recoverable testing>>
  section should be used.

TIP: It is possible to recover fairly easily, using the script as a guide,
from most operations performed by the script if they are accidentally
terminated with a kbd:[Ctrl+C]. A significant exception to this is the
*mke2fs* command. For this reason, the script displays the command to
the terminal to allow the user to cut-and-paste the command to
re-execute it, in case that is ever needed.

[TIP]
====
A recommended monthly back strategy is to do a disk rotation on
both computers. Once the RAIDs on both computers are "recovering" you
can log-out of both computers and then login into the *spare* computer
again to start *refresh_spare_usr2*.

The recovery of the RAIDs will
increase the amount of time that the *refresh_spare_usr2* takes to
complete.  It has been observed in some cases to approximately double
the time required.

Once *refresh_spare_usr2* completes, it is safe to reboot, even if
a recovery is still ongoing. The only requirement is to
reboot the *spare* computer before the FS is run on it again.

A feature of this approach is that will make the *spare* computer
shelf disk a deeper back-up than the *spare* computer RAID disks.

====

==== Using refresh_spare_usr2

NOTE: The purpose of the `script` command below is to record the
output to help verify success afterwards in case the screen output is
no longer available when it is checked for. It can also be helpful for
diagnosing what went wrong if there was a problem.

. As part of a monthly backup, you would usually start a disk rotation
on both the *operational* and *spare* computers first. Once both
computers are recovering, you should log out of both machines.

+

IMPORTANT: Before proceeding, make sure that no one is logged into
either computer and that no processes are running on */usr2* on either
machine, particularly the FS.

. Login on the *spare* computer on a local virtual console text terminal as *root*.

. Execute:
+
    script refresh.txt
    time refresh_spare_usr2; exit
+
Answer the question `y` if it is safe to proceed.
. Check to see that it finished with no problems.
+

If the output of the script is no longer on the display, you can
inspect */refresh.txt* to determine if there was a problem. You may
need to login again to do this.

. If it finished with no problems, you can reboot as soon as is
convenient. You may reboot even if the RAID is recovering, but you can
wait until the recovery is complete. The only requirement is to reboot
before the FS is run again on the *spare* computer.

== Multiple computer set-up

You may have more than one FSL10 computer at a site, either an
*operational* and *spare* for one system and/or additional computers for a
additional systems. In this case, we recommend that you do a full setup of
each computer from scratch from FSL10 installation notes. The main, but not only,
reason for this is to make sure each RAID has a unique UUID, so the
*refresh_secondary* script will be able to help you avoid accidentally
mixing disks while doing a refresh. While in principle is it possible
to do one set-up and clone the configuration to more disks and then
customize for each computer, we are not providing detailed
instructions on how to do that at this time.

It is recommended that the network configuration on each machine be
made independent of the MAC address of the hardware. This will make it
possible to move a RAID pair to a different computer and have it work
on the network. Please note that the IP address and hostname is tied to
the disks and not the computers. For information on how to configure this,
    please see the (optional) <<installation.adoc#_network_configuration_changes,Network configuration changes>> section
    of the FSL10 installation document.

The configuration of the system outside of the */usr2* partition
between *operational* and *spare* computers should be maintained in
parallel so that the same capabilities are available on both. In
particular, any packages installed on one should also be installed
on the other.  In addition, it is important that the user and group
IDs of all users on the operational and spare computers be same. It
should not be necessary to maintain parallelism with OS updates, but that
is recommended as well. It is recommended to maintain maintenance parallelism
with other independent **operational**/**spare** systems at a site as well (this may
    enable additional recovery options in extreme cases).

==  Recovery scenarios

The setup provided by FSL10 provides several layers of recovery in
case of problems with the computers or the disks. Each system has a
*shelf* disk, which can serve as a back-up. Additionally if there is a
*spare* computer for each *operational* computer, there are additional
recovery options. If there are other FSL10 computers at the site, it
may be possible in extreme cases to press those computers and/or disks into
service, particularly if they have been maintained in parallel.

A few example recovery scenarios are described below. In any scenario,
  if disks and/or a computer have failed, they should be repaired or
  replaced as soon as feasible.

=== One disk in the operational computer RAID fails

This should not interrupt operations. The computer should continue to
run seamlessly on the remaining disk.  If the system is rebooted in
this state, it should use the working disk. At the first opportunity,
     usually after operations, the *recover_raid* script can be tried
     to restore the disk to the RAID. If that doesn't work, the disk
     may have failed and may need to replaced (it may worthwhile to
         try blanking and refreshing it first). If the disk has
     failed, it should be removed and a disk rotation should be
     performed (with the still good disk in the *primary* slot) to
     refresh the *shelf* disk and make a working RAID.  The failed
     disk should be repaired or replaced with a new disk that is at
     least as large. The *blank_secondary* script should be used to
     erase the new disk before it is introduced into the rotation
     sequence. See the <<Initialize a new disk>> section above for
     full details on initializing a new disk.

=== Operational computer RAID corrupted

As well as a large scale corruption, this can include recovery from
accidental loss of important non-volatile files. Volatile files
include *.skd*, *.snp*, and *.prc* files (such volatile files can be
    more easily restored by generating them again). It also can be
used to recover from a bad OS patch (which is extremely unlikely),
     which is easier if patches are applied just after a disk
     rotation (see also the <<Recoverable testing>> section).

In this case, the *shelf* disk can be used to restore the system to
the state at the time of the most recent rotation.  To do this, follow
the procedure in <<Recover from a shelf disk>> section above.  The
system can be used for operations once the RAID is recovering for the
first refresh in the procedure.  All needed volatile operational files that were
created/modified after the last disk rotation will need to be
recreated.  Then as time allows, the other disk can recovered by
finishing the procedure in <<Recover from a shelf disk>> section.

If the first disk that is tried for blanking and recovery doesn't work, the
other one can be tried. If neither works, it should be possible to run on just
what was the *shelf* disk until a fuller recovery is possible, probably with
replacements for the malfunctioning disks.

This approach could also be used for a similar problem with the
*spare* computer and using its *shelf* disk for recovery.

This approach of this section should not be used if a problem with the *operational*
computer caused the damage to its RAID. In that case, follow
the <<Operational computer RAID corrupted and operational computer failure>> sub-section below.

=== Operational computer failure

This might be caused by a power supply or other hardware failure.
If the contents of the *operational* RAID are not damaged, the RAID pair
can be moved to the *spare* computer until the *operational* computer is
repaired. Once the RAID has been moved, whether the contents have
been damaged can be assessed. It will be necessary to move
connections for any serial/GPIB devices to the spare computer as well.

=== Operational computer RAID corrupted and operational computer failure

This might happen if the operational computer is exposed to fire
and/or water.  In this case, there are two options. One is switching to
using the *spare* computer as in the <<Loss of operational computer and all its disks>> sub-section below.
The other is to use the *operational* computer's
*shelf* disk in the *spare* computer, either by itself or by making a
ersatz RAID by blanking the *spare* computer's *shelf* disk and
refreshing it from the *operational* computer's *shelf* disk.

In the latter scenario, be sure to preserve the original working RAID
from the *spare* computer. All needed volatile operational files that
were created/modified after the last *operational* computer  disk
rotation will need to be recreated.  It will be necessary to move
connections for any serial/GPIB devices to the spare computer as well.
However, it will not be necessary to enable any daemon's like
*metserver* and *metclient* as it would be in the former scenario; this
may be a significant time saver.

=== Loss of all operational computer disks

If the RAID and *shelf* disk on the *operational* computer are beyond
recovery, the RAID pair from the *spare* computer can be moved to the
*operational* computer. All needed volatile operational files that
were created/modified after the last *refresh_spare_usr2* will need to be
recreated. If daemons like *metserver* and *metclient* are needed,
  they will need to be enabled.

This approach should not be used if a problem with the *operational*
computer caused the damage to its RAID. In that case, follow the
<<Operational computer RAID corrupted and operational computer failure>> sub-section above.

=== Loss of operational computer and all its disks

In this case, operations should be moved to the *spare* computer until
the *operational* computer is repaired or replaced.  It will be
necessary to move connections for any serial/GPIB devices to the
*spare* computer as well. If daemons like *metserver* and
*metclient* are needed, they will need to be enabled. All needed
volatile operational files that were created/modified after the last
*refresh_spare_usr2* will need to be recreated.
