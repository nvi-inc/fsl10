= RAID Notes for FSL 10
W.E. Himwich, J.F.H. Quick,and D.E. Horsley
Draft 0.3 - Jan 2020

:sectnums:
:experimental:
:downarrow: &darr;

:toc:

== Introduction

These notes are intended to cover, albeit tersely, the major issues for
RAID operations with FSL10. The disk lay-out has changed significantly
since FSL9, which required updating the scripts that are used. In
addition, the scripts have been extensively revised to provide more
protection from possible errors in how they are used.

All operations and scripts in this document require *root* privileges
unless otherwise indicated.

== Guidelines for RAID operations

The FSL10 RAID configuration normally uses two disks configured
according to the FSL10 installation instructions. Below are mandatory
and recommended guidelines.

=== Mandatory practices

These practices are fundamental to the operation of the RAID.

. Never mix disks from different computers in one computer.
. Never split up a RAID pair unless already synced, check with *mdstat*
 
A RAID pair (kept in order) can be removed or moved between computers
if need be. A disk rotation or applying updates are probably the only
reasons to split a pair.

=== Recommended practices

These recommendations are intended to provide consistent procedures and make it easier to understand any issues, if they occur.

. Always use the lower number controller (slot) as the *primary*, label it as such, and the other slot as *secondary*
. Make the upper (or left) slot the *primary*, the lower (or right) slot the *secondary*
. Always boot for a refresh/blank with the *primary* slot turned on and the *secondary* slot turned off: so it is clear which is the active disk
. Label the disks (so visible when in use) with the system name and number them 1, 2, and 3, ...
. Label the disks (so visible when in use) with their serial numbers, either from *mdstat* when only one disk inserted or by examining the disk
. For reference, place the disk serial numbers in a file with their corresponding numbers, e.g.:

+
./root/DISKS.txt
[source]
```
1=ZC1B1YCC
2=ZC1A6WZ1
3=ZC1AHENM
```

. When rotating disks, keep the disks in cyclical order (*primary*, *secondary*, *shelf*): 1, 2, 3; then 2, 3, 1;, then 3, 1, 2, then 1, 2, 3; and so on
. Rotate disks for a given computer at least once a month, and before any updates
. If you have a spare computer (and/or additional systems), keep the disks in the same sequence on all the computers
. Do not turn a disk off while the system is running. The only time a key switch state should be changed while the system is running is to add a disk for a blank or refresh operation.

== Disk Rotation

The section describes the disk rotation procedure.

NOTE: Your BIOS must be set to allow hot swapping of disks, particularly for the *secondary* controller,

. Wait until the RAID is not recovering, check with *mdstat*
. Shutdown the system, e.g., *shutdown -h now*
. Take disk from *primary* slot, put on *shelf*
. Move disk from *secondary* slot to *primary* slot, keyed on
. Move old *shelf* disk to *secondary* slot, keyed off
. Boot (*primary* keyed on, *secondary* keyed off)
. Login in as root
. Run *refresh_secondary*
. Key on *secondary* slot when prompted
. If the script rejects the disk (and stops with an error), seek expert advice. Be sure to note any messages so they can be reported.
. If the disk is accepted, let the refresh run to completion (the progress report can be aborted as indicated). The system can be used in the meantime, but may be a little slow.

== Test upgrade of FS - system updates - or other changes

Seek expert advice for this, but the basic plan is given below:

. If a rotation hasn't just been completed, perform one (as an extra backup)
. Before proceeding verify that there is no recovery in progress, check with *mdstat*
. Drop the primary disk out of the RAID (as a backup) using the commands:
+
....
mdadm /dev/md0 -f /dev/sda2
mdadm /dev/md0 -r /dev/sda2
....
. Install and test the update

If the update is deemed *successful*:

[start=5]
. Run *recover_raid* to add the primary disk back into the RAID.
. Once the recover completes, the system has been successfully updated.

Alternatively, If the update is deemed to have *failed*, the system can be
 recovered as follows:

[start=5]
. Shutdown the computer
. Key off the *secondary* slot
. Reboot (*primary* keyed on, *secondary* keyed off)
. Run *blank_secondary*
. Key on *secondary* slot when prompted
. Answer *y* to blank
. Run *refresh_secondary*
. Once the refresh is complete, you have recovered to the pretest state

== Script descriptions

This section describes the various scripts that are used for RAID maintenance.

=== *mdstat*

This script can be used by any user (not just *root*) to check the
status of the RAID. It is most useful for checking whether a recovery
is in process or has ended, but is also useful for showing the current
state of the RAID, including any anomalies.

The script also lists the serial number for all disks that are
present.

=== *refresh_secondary*

This can be used to refresh a *shelf* disk for the RAID as a new
*secondary* disk (*sdb*) as part of a standard three (or more) disk
rotation.

Initially, some sanity checks are performed to confirm that the content the script intends to copy is
where it expects it to be and has the right form.  Any *primary* disk (*sda*) will be rejected that:

. Is not part of the RAID (*md0*)
. Is removable (USB)
. Has a boot scheme other than BIOS or UEFI

For safety reasons, to ensure that only an old *shelf* disk is overwritten,
any *secondary* disk (*sdb*) will be rejected that:

. Was loaded (slot keyed on) before starting the script
+
Unless overridden by *-A* or previously loaded by this or the *blank_secondary* script.

. Is removable (USB)
. Is already part of RAID *md0*

+
Which could only happen when run incorrectly with *-A* (or other
interfering commands have been executed).

. Has a RAID from a different computer, i.e., foreign
+
Technically this could also be another RAID from the same computer, but not of a
properly set up FSL10 computer, which should have only the one RAID

. Has any part already mounted
+
Again catching misuse of the *-A* option.

. Has a different boot scheme than the *primary*
+
And hence is probably from a different computer.

. Has a different RAID UUID
+
This would be a disk from a different computer. Though whether this
check can actually trigger after the test for a foreign RAID above
remains to be seen.

. Was last booted at a future time (possibly due to a mis-set clock or clocks)
. Has had more write activity, i.e., is newer (if the *primary* was just booted, see below)
. Has been booted separately by itself
. Has a different partition layout from the *primary*
. Is smaller than the size of the RAID on the primary disk.

If any of the checks reject the disk, we recommend you seek expert
advice; please record the error report.

The checks are included to make the refresh process as safe as
possible, particular at a station with more than one FSL10 computer.
We believe all the most common errors are trapped, but the script
should still be used with care.

The check on write activity is intended to prevent accidentally using
the *shelf* disk to overwrite a newer disk from the RAID.  This check can be
over-run if the *primary* has run for a considerable period of time
before the refresh is attempted.  This should not be an issue if the
refresh is attempted shortly after the *shelf* disk is booted for the
first time by itself and the RAID was run for more than a trivial
amount of time beforehand.

If the disk being refreshed is from the same computer and has just been
on the *shelf* unused since it was last rotated, it is safe to refresh
and should be accepted by all the checks.

The refresh will take several hours. The script provides a progress
indicator that can safely be aborted (using kbd:[Ctrl+C] as described
    by the on-screen instructions) if that is preferred.  An active
screen saver may make it difficult to see the progress after awhile,
       but pressing kbd:[shift] or some other key should make it
       visible again.  If you abort the progress indicator, you can
       check the progress later with *mdstat*. The system can be used
       normally while it refreshing, but it may be a little slow.

This script requires the *secondary* disk (*sdb*) to not be loaded, i.e.,
the slot turned off, when the script is started. However, it has an
option, *-A* (use only with expert advice), to "Allow" an already
loaded disk to be used. It is intended to make remote operation
possible and must be used with extra care.

If the disk is turned on (when prompted) during the script, it
will automatically be "Allowed" by both this script and
*blank_secondary*, which also supports this feature.  This allows
(expert use only) running *blank_secondary* then rerunning *refresh_secondary*
without having to reboot and cycle the disk off and on for each.

=== *blank_secondary*

This script should only be used with expert advice.

It can be used to make _any_ *secondary* disk (*sdb*) refreshable, if
it is big enough. It must be used with care and only on a *secondary*
disk that you know is safe to erase. Generally speaking you don't want
to use it with a disk from a different FSL10 computer, except for very
unusual circumstances, see <<Recovery scenarios>> for some example
cases. It will ask you to confirm before blanking.

It will reject any *secondary* disk (*sdb*) that:

. Was loaded (slot keyed on) before starting the script
+
Unless you have just loaded it through *refresh_secondary*'s auspices or used
the *-A* option to "Allow" it (see below).

. Is removable (USB)
. Is already part of the RAID *md0*
. Has any partition already mounted
. Has a partition that is in RAID *md0*
+
This is essentially redundant with the first point, but is included
out of an abundance of caution.

. Has a partition that is mounted in any RAID in common with *sda*

This script requires the *secondary* disk (*sdb*) to not be loaded, i.e.,
the slot turned off, when the script is started. However, it has an
option, *-A* (use only with expert advice), to "Allow" an already
loaded disk to be used. It is intended to make remote operation
possible and must be used with extra care.

If the disk is turned on (when prompted) during the script, it will
automatically be "Allowed" by both this script and
*refresh_secondary*, which also supports this feature.  This allows
you to then run *refresh_secondary* without having to reboot and
cycle the disk off and on.

=== *recover_raid*

This script is only for use with expert advice.

This script can be used to recover a disk (*sda* or *sdb*) that has
fallen out of RAID or been marked as faulty either by hand or due to
disk errors.

This script must only be used with a pair of disks that were most
recently used _together_ in an active RAID. It is okay to use it in
this case even if the *failed* disk fell out the RAID a long time ago
and/or there have been extensive changes on the *working* disk.  It is
okay to use if the system was rebooted (even multiples times) or the
*working* disking was booted by itself since the *failed* disk fell
out of the RAID.  However, this script must not be used if the
*failed* disk was booted by itself or to refresh a *shelf* disk from
the *working* disk. It is recommended to only use this script if the
key switches for the disks have not been manipulated since the
*failed* disk fell out of the RAID; in this case it should always be
safe.

It normally works on *md0*, but a different *md* device can be specified as the first argument.

It will reject disks if the RAID:

. Does not need recovery
. Is not in a recoverable state

The recovery may be fairly quick, as short as a few minutes, if the
disk is relatively fresh. You can check the progress with *mdstat*. The
system can be used normally while it recovering, but it may be a
little slow.

=== refresh_spare_usr2

This script is not part of RAID operations per se, but is included in
this document for completeness. In a two computer configuration
(operational and spare), it is used to make a copy of the operational
computer's */usr2* partition on the *spare* computer.

The script can be found in */root/fsl10/RAID*. It should installed on
the *spare* computer *only*.  Full instructions for its installation are
included in the script.

To use this script, It is important that the user and group IDs of all
users on the two systems be the same.

It is recommended that the script be used (including initial test)
  immediately after a disk rotation to provide the ample opportunities
  for recovery if there is a problem. In particular, for initial
  testing the procedure in the <<Test upgrade of FS - system updates - or other changes>>
  section should be used.

#TODO: modify script to echo mke2fs command to aid recovery from inadvertent Control-C and comment here.#

== Multiple computer set-up

You may have more than one FSL10 computer at a site, either an
*operational* and *spare* for one system and/or additional computers for a
additional systems. In this case, we recommend that you do a full setup of
each computer from scratch from FSL10 installation notes. The main, but not only,
reason for this is to make sure each RAID has a unique UUID, so the
*refresh_secondary* script will be able to help you avoid accidentally
mixing disks while doing a refresh. While in principle is it possible
to do one set-up and clone the configuration to more disks and then
customize for each computer, we are not providing detailed
instructions on how to do that at this time.

It is recommended that the network configuration on each machine be
made independent of the MAC address of the hardware. This will make it
possible to move a RAID pair to a different computer and have it work
on the network. Please note that the IP address and node is tied to
the disks and not the computers. For information on how to configure this,
    please see the (optional) *Network configuration changes* section
    of the FSL10 installation document.

The configuration of the system outside of the */usr2* partition
between *operational* and *spare* computers should be maintained in
parallel so that the same capabilities are available on both. In
particular, any packages installed on one should also be installed
on the other.  In addition, it is important that the user and group
IDs of all users on the operational and spare computers be same. It
should not be necessary to maintain parallelism with patches, but that
is recommended as well. It is recommended to maintain this parallelism
between multiple operational/spare systems at a site as well (this may
    enable additional recovery options in extreme cases).

==  Recovery scenarios

The setup provided by FSL10 provides several layers of recovery in
case of problems with the computers or the disks. Each system has a
*shelf* disk, which can serve as a back-up. Additionally if there is a
*spare* computer for each *operational* computer, there are additional
recovery options. If there are other FSL10 computers at the site, it
may be possible in extreme cases to press those computers and/or disks into
service, particularly if they have been maintained in parallel.

A few example recovery scenarios are described below. In any scenario,
  if disks and/or a computer have failed, they should be repaired or
  replaced as soon as feasible.

=== One disk in the RAID fails

This should not interrupt operations. The computer should continue to
run seamlessly on the remaining disk.  If the system is rebooted in
this state, it should use the working disk. At the first opportunity,
     usually after operations, the *recover_raid* script can be tried
     to restore the disk to the RAID. if that doesn't work, the disk
     may have failed and may need to replaced (it may worthwhile to
         try blanking and refreshing it first). If the disk has
     failed, it should be removed and a disk rotation should be
     performed to refresh the *shelf* disk and make a working RAID.
     The failed disk should be repaired or replaced with a new disk
     that is at least as large. The *blank_secondary* script should be
     used too erase the new disk when it is introduced into the
     rotation sequence.

=== *Operational* computer RAID corrupted

As well as a large scale corruption, this can include recovery from
accidental loss of important non-volatile files. Volatile files
include *.skd*, *.snp*, and *.prc* files (such volatile files can be
    more easily restored by generating them again). It also can be
used to recover from a bad OS patch (which is extremely unlikely),
     which is easier if patches are applied just after a disk
     rotation (see also <<Test upgrade of FS - system updates - or other changes>>).

In this case, the *shelf* disks can be used to restore the system to
the state at the time of the most recent rotation.  To do this, boot
with only the *shelf* disk installed in the *primary* slot, then use
the *blank_secondary* script to erase the corrupted disk that is next
in cyclic order, then use *refresh_secondary* to restore a working
RAID.  The system can be used for operations once the RAID is recovering. All needed
volatile operational files that were created/modified after the last disk
rotation will need to be recreated.  Then as time allows, a disk
rotation with the other corrupted disk will bring the system back to
full redundancy and restore the same disk sequence as before.

If the first disk that is tried for blanking and recovery doesn't work, the
other one can be tried. If neither works, it should be possible to run on just
what was the *shelf* disk until a fuller recovery is possible.

This approach could also be used for a similar
problem with the *spare* computer and using its *shelf* disk for
recovery.

This approach of this section should not be used if a problem with the *operational*
computer caused the damage to its RAID. In that case, follow
<<Operational computer RAID corrupted and operational computer failure>>.

=== *Operational* computer failure

This might be caused by a power supply or other hardware failure.
If the contents of the *operational* RAID are not damaged, the RAID pair
can be moved to the *spare* computer until the *operational* computer is
repaired. Once the RAID has been moved, whether the contents have
been damaged can be assessed. It will be necessary to move
connections for any serial/GPIB devices to the spare computer as well.

=== Operational computer RAID corrupted and operational computer failure

This might happen if the operational computer is exposed to fire
and/or water.  In this case, there are two options. One is switching to
using the *spare* computer as in <<Loss of *operational* computer and all its disks>>.
The other is to use the *operational* computer's
*shelf* disk in the *spare* computer, either by itself or by making a
ersatz RAID by blanking the *spare* computer's *shelf* disk and
refreshing it from the *operational* computer's *shelf* disk.

In the latter scenario, all needed volatile operational files that
were created/modified after the last *operational* computer  disk rotation will
need to be recreated.  It will be necessary to move connections for
any serial/GPIB devices to the spare computer as well.  However, it
will not be necessary to enable any daemon's like *metserver* and
*metclient* as it would in the former scenario; this may be significant time
saver.

=== Loss of all *operational* computer disks

If the RAID and *shelf* disk on the *operational* computer are beyond
recovery, the RAID pair from the *spare* computer can be moved to the
*operational* computer. All needed volatile operational files that
were created/modified after the last *refresh_spare_usr2* will need to be
recreated. If daemons like *metserver* and *metclient* are needed,
  they will need to be enabled.

This approach should not be used if a problem with the *operational*
computer caused the damage to its RAID. In that case, follow
<<Operational computer RAID corrupted and operational computer failure>>.

=== Loss of *operational* computer and all its disks

In this case, operations should be moved to the *spare* computer until
the *operational* computer is repaired or replaced.  It will be
necessary to move connections for any serial/GPIB devices to the
*spare* computer as well. If daemons like *metserver* and
*metclient* are needed, they will need to be enabled. All needed
volatile operational files that were created/modified after the last
*refresh_spare_usr2* will need to be recreated.
